experiment_name = MultiChain_crawler

###
### Gumby core configuration options:
###
#
# Directory where to store the output files generated by your experiment and the helper scripts.
workspace_dir = output

tracker_cmd = 'run_tracker.sh'
# tracker_run_remote =
tracker_port = 7788

experiment_server_cmd = 'experiment_server.py'

# Command used to start (the local part of) the experiment. This will be executed concurrently with its remote counterpart if set (see below)
local_instance_cmd = "process_guard.py -c multichain_client.py"

# Command used to post-process the data obtained from the experiment (aggregate data, graph stuff, etc.), this command will be run locally.
# post_process_cmd =

# Process guard timeout in seconds, set to 0 to disable (default: 30)
# 1h = 3600
# 12h = 43200
# 24h = 86400
experiment_time = 3600

###
### scripts/experiment_server.py:
###
#
# Experiment metainfo and time synchronization server.
#
# Config options:
#
# Delay the synchronized start of the experiment by this amount of seconds when giving the start signal.
# The default value should be OK for a few thousand instances. (float, default 5)
# sync_experiment_start_delay = 
#
# Port where we should listen on. (required)
sync_port = __unique_port__
# Override the experiment synchronization server host to which the sync clients will try to connect to (default is HEAD_HOST)
sync_host = 127.0.0.1
#
# Number of sync clients we should wait for to be registered before starting the experiment. (default is DAS4_INSTANCES_TO_RUN)
sync_subscribers_amount = 1

use_local_venv = FALSE

local_output_dir ="output"

# Dir where to write all the output generated from the experiment (default is workspace_dir/output)
 output_dir = "output/multichain"