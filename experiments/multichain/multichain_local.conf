# This is a gumby experiment config file
# Description experiment: Simple gumby experiment to learn gumby.
#
 experiment_name = MultiChain_local

###
### Gumby core configuration options:
###
#
# Directory where to store the output files generated by your experiment and the helper scripts.
workspace_dir = output

#
# Comma separated list of nodes to run the remote_* commands (they can be das4 head nodes or any UNIX host with SSH and rsync installed)
# Take into account that if you use a single node you still need to add a comma at the end.
# head_nodes = node1,node2,node3
#

tracker_cmd = 'run_tracker.sh'
# tracker_run_remote =
tracker_port = 7788

experiment_server_cmd = 'experiment_server.py'

#
# Set this to true if you want the experiment synchronization server to be started on each of the remote nodes.
# Defaults to false
# experiment_server_run_remote =
#
# Command used to locally set up the all the stuff needed to set up the experiment (dependencies, compilations, etc.)
# This will be executed in parallel with the remote counterparts (see below)
# local_setup_cmd =
#
# Same than local_setup_cmd, but executed in each of the remote nodes.
# remote_setup_cmd =
#
# Command used to start (the local part of) the experiment. This will be executed concurrently with its remote counterpart if set (see below)
local_instance_cmd = "process_guard.py -c multichain_client.py  -c multichain_client.py  -c multichain_client.py  -c multichain_client.py -c multichain_client.py"

#
# Command used to start (the remote part of) the experiment. This will be executed concurrently with its local counterpart if set (see above)
# remote_instance_cmd =
#
# Command used to post-process the data obtained from the experiment (aggregate data, graph stuff, etc.), this command will be run locally.
# post_process_cmd =
#
# Use a virtualenv locally? (see http://www.virtualenv.org/en/latest/virtualenv.html if you don't know what this is)
# Defaults to true
# use_local_venv =
#
# Virtualenv to use
# Defaults to $HOME/venv
# virtualenv_dir =
#
###
### Other stuff
###
#
# Gumby already offers a bunch of scripts that will allow you to quickly set up an experiment.
#
# From this point on you can find an auto-generated list of available scripts and its options available at the moment
# of creation of this config file. Take into account that some of the config options are required if you are going to use it.
#


###
### gumby/instrumentation.py:
###
#
# Config options:
#
# 2323)
# manhole_portl port that manhole should listen to. (default = 
#
# Memory dump interval. (default: 60)
# profile_memory_interval = 
#
# Enable memory profiling for the python processes that call instrumentation.init_instrumentation() (default: false)
# profile_memory = 
#
# Space separated list of object types to generate a backref graph from (default: nothing)
# profile_memory_graph_backref_types = 
#
# Enable manhole (telnet access to the python processes), for debugging purposes. User: gumby, pass is empty (default: false)
# manhole_enable = 
#
# Amount of randomly selected objects to graph (default: 1)
# profile_memory_graph_backref_amount = 
#
# Only start the memory dumper for (aproximately) one out of N processes. (default: all processes)
# profile_memory_pid_modulo = 
#


###
### experiments/libswift/build_experiment.sh:
###
#
# Prepare all the necessary stuff for the libswift experiment.
# Does the following:
# - Create the output directories for seeder (src) and leechers (dst)
# - Creates a data file to seed
# - Checks if the previous experiment was cleaned up correctly
# - Creates and mounts a temporary filesystem for the containers
# - Checks out and builds libswift
#
# Config options:
#
# Directory to use for the lxc container (e.g., /tmp/debian-libswift) note the remark in README.MD concerning the sudoers file
# container_dir = 
#
# Size of the file to seed. (e,g., 10M - for syntax see man truncate)
# file_size = 
#


###
### experiments/libswift/graph_libswift_data.sh:
###
#
# Config options:
#
# Override the default stderr parser script (default: TODO).
# libswift_stderr_parser_cmd = 
#


###
### experiments/libswift/leecher_container_cmd.sh:
###
#
# Starts a libswift leecher (from run_experiment.sh), connects to a seeder and downloads a file. Note: runs inside a container.
# start_seeder.sh must be started first.
#


###
### experiments/libswift/run_experiment.sh:
###
#
# Executes libswift experiment. Note that build_experiment.sh should be run before this.
# Starts 1 seeder in a container and connects $NO_OF_LEECHERS leechers to it to download a file.
#
# Config options:
#
# Netem delay for the leechers. Note: for a homogeneous network of leechers, set 1 value
# for a heterogeneous network separate values by , e.g. netem_delay = "0ms,100ms"
# for variation in delay, separate config option with _, e.g. netem_delay = "0ms_10ms,100ms"
# netem_delay = 
#
# Packet loss in % (can also be hetero/homogeneous) for leechers.
# set 1 value, for a heterogeneous network separate values by ,
# e.g. leecher_offset="0%,5%" (note that the number of elements should then match the number of leechers)
# netem_packet_loss = 
#
# Store libswift debug output (optional, default = false).
# debug_swift = 
#
# Last part of IP of first leecher. Will be incremented for additional leechers (e.g., 111)
# leecher_id = 
#
# Process guard timeout in seconds, set to 0 to disable (default: 30)
experiment_time = 900
#
# Time in seconds between startup of leechers. Note: for a homogeneous network of leechers,
# set 1 value, for a heterogeneous network separate values by ,
# e.g. leecher_offset="0,100" (note that the number of elements should then match the number of leechers)
# leecher_offset = 
#
# First part of IP of local network to use for leecher IPs (e.g., 192.168.1)
# network_ip_range = 
#
# # Rate upload limit for leechers (e.g., 100mbit). Note: for a homogeneous network of leechers,
# set 1 value, for a heterogeneous network separate values by ,
# e.g. leecher_offset="1mbit,100mbit" (note that the number of elements should then match the number of leechers)
# netem_rate_ul = 
#
# Time a leecher will remain running (optional, default = exit on complete download),
# set 1 value, for a heterogeneous network separate values by ,
# e.g. leecher_time="100s,200s" (note that the number of elements should then match the number of leechers)
# leecher_time = 
#
# Store ledbat debug output (optional, default = false).
# debug_ledbat = 
#
# Number of leechers to start (default 1)
# no_of_leechers = 
#
# # Rate download limit for leechers (e.g., 100mbit). Note: for a homogeneous network of leechers,
# set 1 value, for a heterogeneous network separate values by ,
# e.g. leecher_offset="1mbit,100mbit" (note that the number of elements should then match the number of leechers)
# netem_rate = 
#


###
### experiments/libswift/seeder_container_cmd.sh:
###
#
# Starts a libswift leecher (from run_experiment.sh), connects to a seeder and downloads a file. Note: runs inside a container.
# start_seeder.sh must be started first.
#
# Config options:
#
# Download rate limit for the seeder. Configure the rate as rate_burst, so e.g. seeder_rate="1mbit_100k"
# seeder_rate = 
#
# Set to true to use iperf test, otherwise swift seeder is started.
# iperf_test = 
#
# Netem delay for the seeder.
# seeder_delay = 
#
# Upload rate limit for the seeder. Configure the rate as rate_burst, so e.g. seeder_rate_ul="1mbit_100k"
# seeder_rate_ul = 
#


###
### experiments/libswift/start_seeder.sh:
###
#
# Config options:
#
# IP of the network bridge of the host (e.g., 192.168.1.20).
# bridge_ip = 
#
# Store libswift debug output (optional, default = false).
# debug_swift = 
#
# Full IP of seeder (e.g., 192.168.1.110)
# seeder_ip = 
#
# Name of the network bridge of the host (e.g., br0).
# bridge_name = 
#
# Port for the seeder (e.g., 2000)
# seeder_port = 
#
# Store ledbat debug output (optional, default = false).
# debug_ledbat = 
#


###
### scripts/build_virtualenv.sh:
###
#
# Builds a virtualenv with everything necessary to run Tribler/Dispersy.  If dtrace is available in the system, it
# also builds SystemTap and a SystemTap-enabled python 2.7 environment.  Can be safely executed every time the
# experiment is run as it will detect if the environment is up to date and exit if there's nothing to do.  Be aware
# that due to SystemTap needing root permissions, the first run of the script will fail giving instructions to the
# user on how to manually run a couple of commands as root to give the necessary privileges to its binaries.
#


###
### scripts/buildswift.sh:
###
#
# Build the Swift binary.
# The script will look in tribler/Tribler/SwiftEngine/ and swift/
# and build the first one it finds. If found in the first location (as in the usual Tribler checkout)
# the resulting binary will be moved to the location Tribler expects to find it.
#
# Config options:
#
# Set to true if you want to enable Swift's debug output. (default is disabled)
# debug_swift = 
#


###
### scripts/das4_node_run_job.sh:
###
#
# Config options:
#
# Time in seconds to wait for the sub-processes to run before killing them. (required)
# das4_node_timeout = 
#
# The command that will be repeatedly launched in the worker nodes of the cluster. (required)
# das4_node_command = 
#


###
### scripts/das4_reserve_and_run.sh:
###
#
# A simple script to run experiments on the DAS4 trough prun.
# Have in mind that this script uses das4_node_run_job.sh, so you will need to set its config options too.
#
# Config options:
#
# Set the reservation time length in seconds. (default is DAS4_NODE_TIMEOUT+120)
# das4_reserve_duration = 
#
# Override the head host where the worker nodes will sync their datasets back (default is the host name where the script is executed from)
# head_host = 
#

#
# Set the number of nodes that will get reserved on each cluster to run this experiment. (required)
# das4_node_amount = 
#


###
### scripts/das4_setup.sh:
###
#
# This script takes care of setting everything up to run a Dispersy/Tribler experiment on the DAS4.
#


###
### scripts/experiment_server.py:
###
#
# Experiment metainfo and time synchronization server.
# TODO: Document this a bit more.
#
# Config options:
#
# Delay the synchronized start of the experiment by this amount of seconds when giving the start signal.
# The default value should be OK for a few thousand instances. (float, default 5)
# sync_experiment_start_delay = 
#
# Port where we should listen on. (required)
sync_port = __unique_port__
# Override the experiment synchronization server host to which the sync clients will try to connect to (default is HEAD_HOST)
sync_host = 127.0.0.1
#
# Number of sync clients we should wait for to be registered before starting the experiment. (default is DAS4_INSTANCES_TO_RUN)
sync_subscribers_amount = 5



###
### scripts/graph_process_guard_data.sh:
###
#
# This script will generate all the resource usage graphs from process_guard.py's output files found in OUTPUT_DIR.
#


###
### scripts/post_process_dispersy_experiment.sh:
###
#
# Config options:
#
# Output dir for local running experiments (so not on DAS4).
local_output_dir ="output"
#
# Override the default statistics extraction script.
# dispersy_statistics_extraction_cmd = 
#


###
### scripts/run_in_env.py:
###
#
# Shell script to run commands inside the experiment environment. Enabling virtualenv if necessary, loading all the needed variables, etc.
# Used by gumby, you shouldn't need to use it directly.
# This script will make the following environment variables available to its subprocesses:
# - PROJECT_DIR: Absolute path to the root of the workspace where gumby and the rest of stuff is.
# - EXPERIMENT_DIR: Absolute path to the directory which contains the experiment config.
# - OUTPUT_DIR: Absolute path to the directory where all the data generated by the experiment execution should be written to.
#
# Config options:
#
# Virtual env to activate for the experiment (default is ~/venv)
# virtualenv_dir = 
#
# Dir where to write all the output generated from the experiment (default is workspace_dir/output)
 output_dir = "output/multichain"



###
### scripts/run_nosetests_for_jenkins.sh:
###
#
# This script runs the tests passed as argument using nose with all the
# flags needed to generate all the data to generate the reports used in
# the jenkins experiments.
#
# Config options:
#
# Specify which tests to run in nose syntax. (default is everything nose can find from within NOSE_RUN_DIR)
# nose_tests_to_run = 
#
# Specify from which directory nose should run (default is $PWD)
# nose_run_dir = 
#


###
### scripts/run_tracker.sh:
###
#
# Starts a dispersy tracker.
#
# Config options:
#
# Enable profiling for the tracker? (default: FALSE)
# tracker_profile = 
#
# Set the port to be used by the tracker. (required)
# tracker_port = 
#
# Listen only on the specified IP. (default: dispersy's default 0.0.0.0)
# tracker_ip = 
#
# Set the type of crypto to be used by the tracker. (default is ECCrypto)
# tracker_crypto = 
#


###
### scripts/stap_ingest_revision_runs.sh:
###
#
# Config options:
#
# # Name of project in the tribler repository (for linking to github)
# toolname = 
#


###
### scripts/tribler_experiment_setup.sh:
###
#
# This setup script should be used for any experiment involving Tribler.
#
# Config options:
#
# Set to any value if your experiment needs swift. (default is disabled)
# build_swift = 
#


###
### scripts/wrap_in_vnc.sh:
###
#
# This script starts a new VNC server and executes the command passed as an argument.
# When the command finishes, it kills the vnc server before exiting.
# apt-get install vnc4server  before running this.
#
